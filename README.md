# Отчет по обучению RL-агента: LunarLander-v3

## 1. Описание задачи
**Цель:** Обучить агента с подкреплением (RL) для решения задачи посадки лунного модуля в среде `LunarLander-v3` и провести сравнительный анализ алгоритмов и архитектур.

**Среда:** `LunarLander-v3` (Gymnasium)
*   **Задача:** Посадить модуль в зону между двумя желтыми флагами.
*   **Действия:** Дискретные (4): ничего не делать, левый двигатель, основной двигатель, правый двигатель.
*   **Награда:** Начисляется за приближение к земле, касание ногами, вычитается за использование топлива. Решением считается средняя награда > 200.

## 2. Методология и эксперименты
Для обучения использовалась библиотека `stable-baselines3`. Было проведено два контролируемых эксперимента с фиксированным `seed=42` и длительностью обучения 110,000 шагов (timesteps).

### Эксперимент 1: Сравнение алгоритмов (PPO vs A2C)
*   **PPO (Proximal Policy Optimization):** Алгоритм on-policy, считающийся стандартом благодаря стабильности.
*   **A2C (Advantage Actor Critic):** Синхронный вариант Actor-Critic.
*   **Гипотеза:** Ожидалось, что PPO покажет лучший результат благодаря механизму клиппинга (clipping), который предотвращает резкие изменения политики.

### Эксперимент 2: Влияние архитектуры нейросети
*   **PPO Default:** Стандартная полносвязная сеть (MLP) с двумя слоями по 64 нейрона (`[64, 64]`).
*   **PPO LargeNet:** Увеличенная сеть с двумя слоями по 256 нейронов (`[256, 256]`).
*   **Гипотеза:** Увеличенная сеть позволит выучить более сложные паттерны управления, что приведет к более плавной посадке.

## 3. Результаты

### График обучения
Ниже представлен график зависимости средней награды от количества шагов обучения (сглаживание окном 50 эпизодов).

![Кривые обучения](rl_plots/training_comparison.png)
*(График показывает, что A2C демонстрирует уверенный рост, в то время как PPO выходит на плато)*

### Количественная оценка (после обучения)
Средняя награда рассчитана по 10 тестовым эпизодам:

| Модель | Средняя награда | Std Dev | Комментарий |
| :--- | :--- | :--- | :--- |
| **A2C Default** | **113.17** | +/- 85.2 | **Лучший результат.** Агент научился садиться. |
| PPO Default | -31.34 | +/- 20.5 | Агент научился не падать, но зависает. |
| PPO LargeNet | -97.26 | +/- 45.1 | Обучение не сошлось, сеть слишком велика. |

## 4. Анализ и выводы

1.  **Победа A2C:** Вопреки гипотезе, алгоритм A2C показал лучший результат на данной дистанции обучения. Он смог найти стратегию посадки и достиг положительной награды. Это может быть связано с тем, что для простых сред A2C иногда обучается быстрее, либо выбранный `seed=42` оказался удачным для инициализации весов A2C.
2.  **Локальный минимум PPO:** Базовый PPO застрял на уровне награды `-30`. Это классическая проблема "локального минимума": агент научился безопасно зависать в воздухе или медленно снижаться, избегая сильных штрафов за аварию, но не нашел способ получить большую награду за точную посадку. Для исправления можно увеличить коэффициент энтропии (entropy coefficient) для стимулирования исследования.
3.  **Проблема большой сети:** Гипотеза о пользе большой сети (`256x256`) не подтвердилась. Для задачи LunarLander пространство состояний не настолько сложное, чтобы требовать такого количества параметров. Большая сеть обучалась медленнее и требовала больше данных (sample efficiency), чем было предоставлено (110k шагов).

**Итог:** Для простых задач (как LunarLander) более простые модели и алгоритмы могут показывать лучшие результаты при ограниченном бюджете обучения.

## 5. Визуализация
Примеры работы агентов сохранены в папке `rl_videos/`.
*   `A2C_Default.mp4` — демонстрирует успешную (или почти успешную) посадку.
*   `PPO_Default.mp4` — демонстрирует зависание или неоптимальную траекторию.

## 6. Воспроизводимость
Для запуска кода и повторения экспериментов:

1.  Установите зависимости:
    ```bash
    pip install "gymnasium[box2d]" stable-baselines3 shimmy moviepy matplotlib pandas imageio
    ```
2.  Запустите ячейку в ноутбуке
3.  Параметры среды:
    *   `ENV_ID = "LunarLander-v3"`
    *   `SEED = 42`

    *   `TOTAL_TIMESTEPS = 110000`

